{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(123)\n",
    "#from tensorflow import set_random_seed\n",
    "#set_random_seed(234)\n",
    "\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn import decomposition\n",
    "import scipy\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Layer, InputSpec\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers, activations, initializers, constraints, Sequential\n",
    "from keras import backend as K\n",
    "from keras.constraints import UnitNorm, Constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate random multi-dimensional correlated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**. Set the dimension of the data.\n",
    "\n",
    "We set the dim small to clear understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2.1.** Generate a positive definite symmetric matrix to be used as covariance to generate a random data.\n",
    "\n",
    "This is a matrix of size n_dim x n_dim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = sklearn.datasets.make_spd_matrix(n_dim, random_state=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2.2.** Generate a vector of mean for generating the random data.\n",
    "\n",
    "This is an np array of size n_dim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.random.normal(0, 0.1, n_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**. Generate the random data, `X`.\n",
    "\n",
    "The number of samples for `X` is set as `n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "\n",
    "X = np.random.multivariate_normal(mu, cov, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4.** Split the data into train and test.\n",
    "\n",
    "We split the data into train and test. The test will be used to measure the improvement in Autoencoder after tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(X, test_size=0.5, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52681732, 0.62810095, 0.59284649, 0.52236607, 0.40995414],\n",
       "       [0.41440755, 0.55001423, 0.63635182, 0.57413981, 0.30587165],\n",
       "       [0.38378646, 0.14857921, 0.39851437, 0.34588993, 0.63374338],\n",
       "       ...,\n",
       "       [0.28371994, 0.4794476 , 0.48410551, 0.54245635, 0.59818998],\n",
       "       [0.50510169, 0.76450508, 0.73271896, 0.69792451, 0.34080318],\n",
       "       [0.3978237 , 0.50478323, 0.40141433, 0.67971132, 0.60012005]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA vs Single Layer Linear Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = decomposition.PCA(n_components=2)\n",
    "\n",
    "pca.fit(X_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Single Layer Linear Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 2)                 12        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 15        \n",
      "=================================================================\n",
      "Total params: 27\n",
      "Trainable params: 27\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "500/500 [==============================] - 0s 429us/step - loss: 0.6180 - accuracy: 0.0540\n",
      "Epoch 2/100\n",
      "500/500 [==============================] - 0s 54us/step - loss: 0.2371 - accuracy: 0.1540\n",
      "Epoch 3/100\n",
      "500/500 [==============================] - 0s 54us/step - loss: 0.1403 - accuracy: 0.4360\n",
      "Epoch 4/100\n",
      "500/500 [==============================] - 0s 53us/step - loss: 0.1003 - accuracy: 0.5400\n",
      "Epoch 5/100\n",
      "500/500 [==============================] - 0s 54us/step - loss: 0.0778 - accuracy: 0.5500\n",
      "Epoch 6/100\n",
      "500/500 [==============================] - 0s 55us/step - loss: 0.0635 - accuracy: 0.5440\n",
      "Epoch 7/100\n",
      "500/500 [==============================] - 0s 55us/step - loss: 0.0538 - accuracy: 0.5400\n",
      "Epoch 8/100\n",
      "500/500 [==============================] - 0s 53us/step - loss: 0.0471 - accuracy: 0.5380\n",
      "Epoch 9/100\n",
      "500/500 [==============================] - 0s 55us/step - loss: 0.0422 - accuracy: 0.5420\n",
      "Epoch 10/100\n",
      "500/500 [==============================] - 0s 54us/step - loss: 0.0388 - accuracy: 0.5460\n",
      "Epoch 11/100\n",
      "500/500 [==============================] - 0s 53us/step - loss: 0.0364 - accuracy: 0.5500\n",
      "Epoch 12/100\n",
      "500/500 [==============================] - 0s 54us/step - loss: 0.0347 - accuracy: 0.5480\n",
      "Epoch 13/100\n",
      "500/500 [==============================] - 0s 53us/step - loss: 0.0334 - accuracy: 0.5480\n",
      "Epoch 14/100\n",
      "500/500 [==============================] - 0s 57us/step - loss: 0.0325 - accuracy: 0.5460\n",
      "Epoch 15/100\n",
      "500/500 [==============================] - 0s 54us/step - loss: 0.0319 - accuracy: 0.5460\n",
      "Epoch 16/100\n",
      "500/500 [==============================] - 0s 55us/step - loss: 0.0314 - accuracy: 0.5440\n",
      "Epoch 17/100\n",
      "500/500 [==============================] - 0s 54us/step - loss: 0.0311 - accuracy: 0.5400\n",
      "Epoch 18/100\n",
      "500/500 [==============================] - 0s 51us/step - loss: 0.0308 - accuracy: 0.5460\n",
      "Epoch 19/100\n",
      "500/500 [==============================] - 0s 54us/step - loss: 0.0305 - accuracy: 0.5460\n",
      "Epoch 20/100\n",
      "500/500 [==============================] - 0s 55us/step - loss: 0.0303 - accuracy: 0.5440\n",
      "Epoch 21/100\n",
      "500/500 [==============================] - 0s 55us/step - loss: 0.0302 - accuracy: 0.5440\n",
      "Epoch 22/100\n",
      "500/500 [==============================] - 0s 55us/step - loss: 0.0301 - accuracy: 0.5380\n",
      "Epoch 23/100\n",
      "500/500 [==============================] - 0s 57us/step - loss: 0.0300 - accuracy: 0.5460\n",
      "Epoch 24/100\n",
      "500/500 [==============================] - 0s 53us/step - loss: 0.0299 - accuracy: 0.5420\n",
      "Epoch 25/100\n",
      "500/500 [==============================] - 0s 53us/step - loss: 0.0298 - accuracy: 0.5420\n",
      "Epoch 26/100\n",
      "500/500 [==============================] - 0s 55us/step - loss: 0.0297 - accuracy: 0.5440\n",
      "Epoch 27/100\n",
      "500/500 [==============================] - 0s 54us/step - loss: 0.0296 - accuracy: 0.5460\n",
      "Epoch 28/100\n",
      "500/500 [==============================] - 0s 55us/step - loss: 0.0295 - accuracy: 0.5460\n",
      "Epoch 29/100\n",
      "500/500 [==============================] - 0s 53us/step - loss: 0.0294 - accuracy: 0.5460\n",
      "Epoch 30/100\n",
      "500/500 [==============================] - 0s 52us/step - loss: 0.0293 - accuracy: 0.5460\n",
      "Epoch 31/100\n",
      "500/500 [==============================] - 0s 53us/step - loss: 0.0292 - accuracy: 0.5480\n",
      "Epoch 32/100\n",
      "500/500 [==============================] - 0s 53us/step - loss: 0.0291 - accuracy: 0.5480\n",
      "Epoch 33/100\n",
      "500/500 [==============================] - 0s 55us/step - loss: 0.0291 - accuracy: 0.5480\n",
      "Epoch 34/100\n",
      "500/500 [==============================] - 0s 55us/step - loss: 0.0290 - accuracy: 0.5520\n",
      "Epoch 35/100\n",
      "500/500 [==============================] - 0s 55us/step - loss: 0.0289 - accuracy: 0.5500\n",
      "Epoch 36/100\n",
      "500/500 [==============================] - 0s 52us/step - loss: 0.0288 - accuracy: 0.5520\n",
      "Epoch 37/100\n",
      "500/500 [==============================] - 0s 52us/step - loss: 0.0287 - accuracy: 0.5520\n",
      "Epoch 38/100\n",
      "500/500 [==============================] - 0s 52us/step - loss: 0.0286 - accuracy: 0.5540\n",
      "Epoch 39/100\n",
      "500/500 [==============================] - 0s 51us/step - loss: 0.0285 - accuracy: 0.5540\n",
      "Epoch 40/100\n",
      "500/500 [==============================] - 0s 52us/step - loss: 0.0285 - accuracy: 0.5540\n",
      "Epoch 41/100\n",
      "500/500 [==============================] - 0s 53us/step - loss: 0.0284 - accuracy: 0.5560\n",
      "Epoch 42/100\n",
      "500/500 [==============================] - 0s 54us/step - loss: 0.0283 - accuracy: 0.5580\n",
      "Epoch 43/100\n",
      "500/500 [==============================] - 0s 54us/step - loss: 0.0282 - accuracy: 0.5620\n",
      "Epoch 44/100\n",
      "500/500 [==============================] - 0s 53us/step - loss: 0.0281 - accuracy: 0.5600\n",
      "Epoch 45/100\n",
      "500/500 [==============================] - 0s 52us/step - loss: 0.0281 - accuracy: 0.5580\n",
      "Epoch 46/100\n",
      "500/500 [==============================] - 0s 55us/step - loss: 0.0280 - accuracy: 0.5620\n",
      "Epoch 47/100\n",
      "500/500 [==============================] - 0s 51us/step - loss: 0.0279 - accuracy: 0.5640\n",
      "Epoch 48/100\n",
      "500/500 [==============================] - 0s 49us/step - loss: 0.0278 - accuracy: 0.5640\n",
      "Epoch 49/100\n",
      "500/500 [==============================] - 0s 50us/step - loss: 0.0277 - accuracy: 0.5640\n",
      "Epoch 50/100\n",
      "500/500 [==============================] - 0s 50us/step - loss: 0.0276 - accuracy: 0.5640\n",
      "Epoch 51/100\n",
      "500/500 [==============================] - 0s 50us/step - loss: 0.0276 - accuracy: 0.5640\n",
      "Epoch 52/100\n",
      "500/500 [==============================] - 0s 50us/step - loss: 0.0275 - accuracy: 0.5660\n",
      "Epoch 53/100\n",
      "500/500 [==============================] - 0s 53us/step - loss: 0.0274 - accuracy: 0.5640\n",
      "Epoch 54/100\n",
      "500/500 [==============================] - 0s 55us/step - loss: 0.0273 - accuracy: 0.5660\n",
      "Epoch 55/100\n",
      "500/500 [==============================] - 0s 54us/step - loss: 0.0272 - accuracy: 0.5680\n",
      "Epoch 56/100\n",
      "500/500 [==============================] - 0s 51us/step - loss: 0.0271 - accuracy: 0.5660\n",
      "Epoch 57/100\n",
      "500/500 [==============================] - 0s 50us/step - loss: 0.0271 - accuracy: 0.5700\n",
      "Epoch 58/100\n",
      "500/500 [==============================] - 0s 50us/step - loss: 0.0270 - accuracy: 0.5720\n",
      "Epoch 59/100\n",
      "500/500 [==============================] - 0s 52us/step - loss: 0.0269 - accuracy: 0.5720\n",
      "Epoch 60/100\n",
      "500/500 [==============================] - 0s 50us/step - loss: 0.0268 - accuracy: 0.5720\n",
      "Epoch 61/100\n",
      "500/500 [==============================] - 0s 50us/step - loss: 0.0267 - accuracy: 0.5720\n",
      "Epoch 62/100\n",
      "500/500 [==============================] - 0s 50us/step - loss: 0.0266 - accuracy: 0.5740\n",
      "Epoch 63/100\n",
      "500/500 [==============================] - 0s 50us/step - loss: 0.0266 - accuracy: 0.5800\n",
      "Epoch 64/100\n",
      "500/500 [==============================] - 0s 51us/step - loss: 0.0265 - accuracy: 0.5820\n",
      "Epoch 65/100\n",
      "500/500 [==============================] - 0s 51us/step - loss: 0.0264 - accuracy: 0.5840\n",
      "Epoch 66/100\n",
      "500/500 [==============================] - 0s 53us/step - loss: 0.0263 - accuracy: 0.5860\n",
      "Epoch 67/100\n",
      "500/500 [==============================] - 0s 53us/step - loss: 0.0262 - accuracy: 0.5840\n",
      "Epoch 68/100\n",
      "500/500 [==============================] - 0s 52us/step - loss: 0.0261 - accuracy: 0.5860\n",
      "Epoch 69/100\n",
      "500/500 [==============================] - 0s 52us/step - loss: 0.0261 - accuracy: 0.5900\n",
      "Epoch 70/100\n",
      "500/500 [==============================] - 0s 51us/step - loss: 0.0260 - accuracy: 0.5900\n",
      "Epoch 71/100\n",
      "500/500 [==============================] - 0s 53us/step - loss: 0.0259 - accuracy: 0.5860\n",
      "Epoch 72/100\n",
      "500/500 [==============================] - 0s 53us/step - loss: 0.0258 - accuracy: 0.5900\n",
      "Epoch 73/100\n",
      "500/500 [==============================] - 0s 52us/step - loss: 0.0257 - accuracy: 0.5900\n",
      "Epoch 74/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 53us/step - loss: 0.0256 - accuracy: 0.5860\n",
      "Epoch 75/100\n",
      "500/500 [==============================] - 0s 55us/step - loss: 0.0255 - accuracy: 0.5880\n",
      "Epoch 76/100\n",
      "500/500 [==============================] - 0s 54us/step - loss: 0.0255 - accuracy: 0.5920\n",
      "Epoch 77/100\n",
      "500/500 [==============================] - 0s 55us/step - loss: 0.0254 - accuracy: 0.5900\n",
      "Epoch 78/100\n",
      "500/500 [==============================] - 0s 55us/step - loss: 0.0253 - accuracy: 0.5900\n",
      "Epoch 79/100\n",
      "500/500 [==============================] - 0s 54us/step - loss: 0.0252 - accuracy: 0.5880\n",
      "Epoch 80/100\n",
      "500/500 [==============================] - 0s 55us/step - loss: 0.0251 - accuracy: 0.5860\n",
      "Epoch 81/100\n",
      "500/500 [==============================] - 0s 55us/step - loss: 0.0250 - accuracy: 0.5900\n",
      "Epoch 82/100\n",
      "500/500 [==============================] - 0s 54us/step - loss: 0.0249 - accuracy: 0.5880\n",
      "Epoch 83/100\n",
      "500/500 [==============================] - 0s 53us/step - loss: 0.0248 - accuracy: 0.5880\n",
      "Epoch 84/100\n",
      "500/500 [==============================] - 0s 54us/step - loss: 0.0248 - accuracy: 0.5880\n",
      "Epoch 85/100\n",
      "500/500 [==============================] - 0s 55us/step - loss: 0.0247 - accuracy: 0.5900\n",
      "Epoch 86/100\n",
      "500/500 [==============================] - 0s 57us/step - loss: 0.0246 - accuracy: 0.5900\n",
      "Epoch 87/100\n",
      "500/500 [==============================] - 0s 55us/step - loss: 0.0245 - accuracy: 0.5880\n",
      "Epoch 88/100\n",
      "500/500 [==============================] - 0s 54us/step - loss: 0.0244 - accuracy: 0.5900\n",
      "Epoch 89/100\n",
      "500/500 [==============================] - 0s 52us/step - loss: 0.0243 - accuracy: 0.5880\n",
      "Epoch 90/100\n",
      "500/500 [==============================] - 0s 53us/step - loss: 0.0242 - accuracy: 0.5920\n",
      "Epoch 91/100\n",
      "500/500 [==============================] - 0s 53us/step - loss: 0.0241 - accuracy: 0.5920\n",
      "Epoch 92/100\n",
      "500/500 [==============================] - 0s 57us/step - loss: 0.0240 - accuracy: 0.5900\n",
      "Epoch 93/100\n",
      "500/500 [==============================] - 0s 57us/step - loss: 0.0240 - accuracy: 0.5940\n",
      "Epoch 94/100\n",
      "500/500 [==============================] - 0s 56us/step - loss: 0.0239 - accuracy: 0.5900\n",
      "Epoch 95/100\n",
      "500/500 [==============================] - 0s 56us/step - loss: 0.0238 - accuracy: 0.5920\n",
      "Epoch 96/100\n",
      "500/500 [==============================] - 0s 57us/step - loss: 0.0237 - accuracy: 0.5960\n",
      "Epoch 97/100\n",
      "500/500 [==============================] - 0s 57us/step - loss: 0.0236 - accuracy: 0.5940\n",
      "Epoch 98/100\n",
      "500/500 [==============================] - 0s 57us/step - loss: 0.0235 - accuracy: 0.5960\n",
      "Epoch 99/100\n",
      "500/500 [==============================] - 0s 56us/step - loss: 0.0234 - accuracy: 0.5920\n",
      "Epoch 100/100\n",
      "500/500 [==============================] - 0s 55us/step - loss: 0.0233 - accuracy: 0.5960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f528eebf790>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_epoch = 100\n",
    "batch_size = 16\n",
    "input_dim = X_train_scaled.shape[1] #num of predictor variables,\n",
    "encoding_dim = 2\n",
    "learning_rate = 1e-3\n",
    "\n",
    "encoder = Dense(encoding_dim, activation=\"linear\", input_shape=(input_dim,), use_bias = True)\n",
    "decoder = Dense(input_dim, activation=\"linear\", use_bias = True)\n",
    "\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "\n",
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='sgd')\n",
    "autoencoder                    \n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled,\n",
    "                epochs=nb_epoch,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare and contrast the outputs.\n",
    "\n",
    "### 1. Tied Weights\n",
    "\n",
    "The weights on Encoder and Decoder are not the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder weights \n",
      " [[-0.47  0.35 -0.51 -0.46 -0.32]\n",
      " [-0.68 -0.07  0.8  -0.55 -0.67]]\n",
      "Decoder weights \n",
      " [[ 0.23 -0.79  0.26 -0.59  0.5 ]\n",
      " [ 0.2  -0.06 -0.08 -0.3  -0.7 ]]\n"
     ]
    }
   ],
   "source": [
    "w_encoder = np.round(autoencoder.layers[0].get_weights()[0], 2).T  # W in Figure 2.\n",
    "w_decoder = np.round(autoencoder.layers[1].get_weights()[0], 2)  # W' in Figure 2.\n",
    "print('Encoder weights \\n', w_encoder)\n",
    "print('Decoder weights \\n', w_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Weight Orthogonality\n",
    "Unlike PCA weights, the weights on Encoder and Decoder are not orthogonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_pca = pca.components_\n",
    "np.round(np.dot(w_pca, w_pca.T), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.918, 0.355],\n",
       "       [0.355, 1.859]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.dot(w_encoder, w_encoder.T), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.343, -0.1  ],\n",
       "       [-0.1  ,  0.63 ]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.dot(w_decoder, w_decoder.T), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Uncorrelated Features\n",
    "Unlike PCA features, i.e. Principal Scores, the Encoded features are correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09899, -0.     ],\n",
       "       [-0.     ,  0.01549]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_features = pca.fit_transform(X_train_scaled)\n",
    "np.round(np.cov(pca_features.T), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded feature covariance\n",
      " [[0.01636493 0.0002863 ]\n",
      " [0.0002863  0.01895543]]\n"
     ]
    }
   ],
   "source": [
    "encoder_layer = Model(inputs=autoencoder.inputs, outputs=autoencoder.layers[0].output)\n",
    "encoded_features = np.array(encoder_layer.predict(X_train_scaled))\n",
    "print('Encoded feature covariance\\n', np.cov(encoded_features.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Unit Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA weights norm, \n",
      " [1. 1.]\n",
      "Encoder weights norm, \n",
      " [0.9175 1.8587]\n",
      "Decoder weights norm, \n",
      " [1.3427 0.63  ]\n"
     ]
    }
   ],
   "source": [
    "print('PCA weights norm, \\n', np.sum(w_pca ** 2, axis = 1))\n",
    "print('Encoder weights norm, \\n', np.sum(w_encoder ** 2, axis = 1))\n",
    "print('Decoder weights norm, \\n', np.sum(w_decoder ** 2, axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Reconstruction Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reconstrunction error\n",
      " 0.02325372908429263\n",
      "Test reconstrunction error\n",
      " 0.022643016308100682\n"
     ]
    }
   ],
   "source": [
    "train_predictions = autoencoder.predict(X_train_scaled)\n",
    "print('Train reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_train_scaled, train_predictions))\n",
    "test_predictions = autoencoder.predict(X_test_scaled)\n",
    "print('Test reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_test_scaled, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Well-posed Autoencoder\n",
    "### Constraints for Autoencoder\n",
    "Optimizing Autoencoder using PCA principles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 100\n",
    "batch_size = 16\n",
    "input_dim = X_train_scaled.shape[1] #num of predictor variables,\n",
    "encoding_dim = 2\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Constraint: Tied weights\n",
    "\n",
    "Make decoder weights equal to encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseTied(Layer):\n",
    "    def __init__(self, units,\n",
    "                 activation=None,\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 tied_to=None,\n",
    "                 **kwargs):\n",
    "        self.tied_to = tied_to\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "        self.input_spec = InputSpec(min_ndim=2)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 2\n",
    "        input_dim = input_shape[-1]\n",
    "\n",
    "        if self.tied_to is not None:\n",
    "            self.kernel = K.transpose(self.tied_to.kernel)\n",
    "            self._non_trainable_weights.append(self.kernel)\n",
    "        else:\n",
    "            self.kernel = self.add_weight(shape=(input_dim, self.units),\n",
    "                                          initializer=self.kernel_initializer,\n",
    "                                          name='kernel',\n",
    "                                          regularizer=self.kernel_regularizer,\n",
    "                                          constraint=self.kernel_constraint)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.units,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n",
    "        self.built = True\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) >= 2\n",
    "        output_shape = list(input_shape)\n",
    "        output_shape[-1] = self.units\n",
    "        return tuple(output_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        output = K.dot(inputs, self.kernel)\n",
    "        if self.use_bias:\n",
    "            output = K.bias_add(output, self.bias, data_format='channels_last')\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Bias=False for Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Dense(encoding_dim, activation=\"linear\", input_shape=(input_dim,), use_bias = True)\n",
    "decoder = DenseTied(input_dim, activation=\"linear\", tied_to=encoder, use_bias = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 2)                 12        \n",
      "_________________________________________________________________\n",
      "dense_tied_1 (DenseTied)     (None, 5)                 22        \n",
      "=================================================================\n",
      "Total params: 22\n",
      "Trainable params: 12\n",
      "Non-trainable params: 10\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f528eeed210>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "\n",
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='sgd')\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled,\n",
    "                epochs=3,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder weights\n",
      " [[ 0.62  -0.761 -0.824  0.049  0.521]\n",
      " [ 0.086 -0.727  0.036  0.47  -0.73 ]]\n",
      "Decoder weights\n",
      " [[ 0.62  -0.761 -0.824  0.049  0.521]\n",
      " [ 0.086 -0.727  0.036  0.47  -0.73 ]]\n"
     ]
    }
   ],
   "source": [
    "#w_encoder = np.round(np.transpose(autoencoder.layers[0].get_weights()[0]), 3)\n",
    "#w_decoder = np.round(autoencoder.layers[1].get_weights()[0], 3)\n",
    "#get_weights() not working for class DenseTied because DenseTied.weights contains a tensor element\n",
    "#which\n",
    "w_encoder = np.round(np.transpose(autoencoder.layers[0].weights[0].numpy()), 3)\n",
    "w_decoder = np.round(np.transpose(autoencoder.layers[1].weights[0].numpy()), 3)\n",
    "print('Encoder weights\\n', w_encoder)\n",
    "print('Decoder weights\\n', w_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reconstrunction error\n",
      " 0.21581950338286945\n",
      "Test reconstrunction error\n",
      " 0.21446727007950078\n"
     ]
    }
   ],
   "source": [
    "train_predictions = autoencoder.predict(X_train_scaled)\n",
    "print('Train reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_train_scaled, train_predictions))\n",
    "test_predictions = autoencoder.predict(X_test_scaled)\n",
    "print('Test reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_test_scaled, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Bias=True for Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 2)                 12        \n",
      "_________________________________________________________________\n",
      "dense_tied_2 (DenseTied)     (None, 5)                 27        \n",
      "=================================================================\n",
      "Total params: 27\n",
      "Trainable params: 17\n",
      "Non-trainable params: 10\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f528e99dc90>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Dense(encoding_dim, activation=\"linear\", input_shape=(input_dim,), use_bias = True)\n",
    "decoder = DenseTied(input_dim, activation=\"linear\", tied_to=encoder, use_bias = True)\n",
    "\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "\n",
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='sgd')\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled,\n",
    "                epochs=nb_epoch,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder weights\n",
      " [[ 0.384  0.414  0.606  0.233 -0.48 ]\n",
      " [ 0.038  0.419 -0.732  0.184 -0.466]]\n",
      "Decoder weights\n",
      " [[ 0.384  0.414  0.606  0.233 -0.48 ]\n",
      " [ 0.038  0.419 -0.732  0.184 -0.466]]\n",
      "PCA weights\n",
      " [[ 0.41991315  0.44792337  0.47767246  0.4164908  -0.47053377]\n",
      " [ 0.31253406 -0.13919604  0.32638195 -0.84008385 -0.26585876]]\n"
     ]
    }
   ],
   "source": [
    "#w_encoder = np.round(np.transpose(autoencoder.layers[0].get_weights()[0]), 3)\n",
    "#w_decoder = np.round(autoencoder.layers[1].get_weights()[1], 3)\n",
    "w_encoder = np.round(autoencoder.layers[0].weights[0].numpy().transpose(), 3)\n",
    "w_decoder = np.round(autoencoder.layers[1].weights[1].numpy().transpose(), 3)\n",
    "\n",
    "print('Encoder weights\\n', w_encoder)\n",
    "print('Decoder weights\\n', w_decoder)\n",
    "print('PCA weights\\n', w_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reconstrunction error\n",
      " 0.006114087527982598\n",
      "Test reconstrunction error\n",
      " 0.006267197898881717\n"
     ]
    }
   ],
   "source": [
    "train_predictions = autoencoder.predict(X_train_scaled)\n",
    "print('Train reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_train_scaled, train_predictions))\n",
    "test_predictions = autoencoder.predict(X_test_scaled)\n",
    "print('Test reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_test_scaled, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Constraint: Weights orthogonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightsOrthogonalityConstraint (Constraint):\n",
    "    def __init__(self, encoding_dim, weightage = 1.0, axis = 0):\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.weightage = weightage\n",
    "        self.axis = axis\n",
    "\n",
    "    def weights_orthogonality(self, w):\n",
    "        if(self.axis==1):\n",
    "            w = K.transpose(w)\n",
    "        if(self.encoding_dim > 1):\n",
    "            m = K.dot(K.transpose(w), w) - K.eye(self.encoding_dim)\n",
    "            return self.weightage * K.sqrt(K.sum(K.square(m)))\n",
    "        else:\n",
    "            m = K.sum(w ** 2) - 1.\n",
    "            return m\n",
    "\n",
    "    def __call__(self, w):\n",
    "        return self.weights_orthogonality(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Encoder weight orthogonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 2)                 12        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 15        \n",
      "=================================================================\n",
      "Total params: 27\n",
      "Trainable params: 27\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f24b8518150>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Dense(encoding_dim, activation=\"linear\", input_shape=(input_dim,), use_bias=True, kernel_regularizer=WeightsOrthogonalityConstraint(encoding_dim, weightage=1., axis=0))\n",
    "decoder = Dense(input_dim, activation=\"linear\", use_bias = True)\n",
    "\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "\n",
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='sgd')\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled,\n",
    "                epochs=nb_epoch,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder weights dot product\n",
      " [[0.99 0.02]\n",
      " [0.02 0.98]]\n"
     ]
    }
   ],
   "source": [
    "w_encoder = autoencoder.layers[0].get_weights()[0]\n",
    "print('Encoder weights dot product\\n', np.round(np.dot(w_encoder.T, w_encoder), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reconstrunction error\n",
      " 0.022781762567340248\n",
      "Test reconstrunction error\n",
      " 0.021670269457052582\n"
     ]
    }
   ],
   "source": [
    "train_predictions = autoencoder.predict(X_train_scaled)\n",
    "print('Train reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_train_scaled, train_predictions))\n",
    "test_predictions = autoencoder.predict(X_test_scaled)\n",
    "print('Test reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_test_scaled, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Encoder and Decoder Weight orthogonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 2)                 12        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 5)                 15        \n",
      "=================================================================\n",
      "Total params: 27\n",
      "Trainable params: 27\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f24b8423510>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Dense(encoding_dim, activation=\"linear\", input_shape=(input_dim,), use_bias=True, kernel_regularizer=WeightsOrthogonalityConstraint(encoding_dim, weightage=1., axis=0))\n",
    "decoder = Dense(input_dim, activation=\"linear\", use_bias = True, kernel_regularizer=WeightsOrthogonalityConstraint(encoding_dim, weightage=1., axis=1))\n",
    "\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "\n",
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='sgd')\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled,\n",
    "                epochs=nb_epoch,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder weights dot product\n",
      " [[ 1.01 -0.  ]\n",
      " [-0.    1.  ]]\n",
      "Decoder weights dot product\n",
      " [[ 0.97 -0.01]\n",
      " [-0.01  1.  ]]\n"
     ]
    }
   ],
   "source": [
    "w_encoder = autoencoder.layers[0].get_weights()[0]\n",
    "print('Encoder weights dot product\\n', np.round(np.dot(w_encoder.T, w_encoder), 2))\n",
    "\n",
    "w_decoder = autoencoder.layers[1].get_weights()[0]\n",
    "print('Decoder weights dot product\\n', np.round(np.dot(w_decoder, w_decoder.T), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Constraint: Uncorrelated Encoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UncorrelatedFeaturesConstraint (Constraint):\n",
    "\n",
    "    def __init__(self, encoding_dim, weightage=1.0):\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.weightage = weightage\n",
    "\n",
    "    def get_covariance(self, x):\n",
    "        x_centered_list = []\n",
    "\n",
    "        for i in range(self.encoding_dim):\n",
    "            x_centered_list.append(x[:, i] - K.mean(x[:, i]))\n",
    "\n",
    "        x_centered = tf.stack(x_centered_list)\n",
    "        covariance = K.dot(x_centered, K.transpose(x_centered)) / \\\n",
    "            tf.cast(x_centered.get_shape()[0], tf.float32)\n",
    "\n",
    "        return covariance\n",
    "\n",
    "    # Constraint penalty\n",
    "    def uncorrelated_feature(self, x):\n",
    "        if(self.encoding_dim <= 1):\n",
    "            return 0.0\n",
    "        else:\n",
    "            output = K.sum(K.square(\n",
    "                self.covariance - tf.math.multiply(self.covariance, K.eye(self.encoding_dim))))\n",
    "            return output\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.covariance = self.get_covariance(x)\n",
    "        return self.weightage * self.uncorrelated_feature(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 2)                 12        \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 5)                 15        \n",
      "=================================================================\n",
      "Total params: 27\n",
      "Trainable params: 27\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f24b84c5390>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Dense(encoding_dim, activation=\"linear\", input_shape=(input_dim,), use_bias=True,\n",
    "                activity_regularizer=UncorrelatedFeaturesConstraint(encoding_dim, weightage=1.))\n",
    "decoder = Dense(input_dim, activation=\"linear\", use_bias=True)\n",
    "\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "\n",
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='sgd')\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled,\n",
    "                epochs=nb_epoch,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded feature covariance\n",
      " [[ 0.006 -0.   ]\n",
      " [-0.     0.019]]\n"
     ]
    }
   ],
   "source": [
    "encoder_layer = Model(inputs=autoencoder.inputs, outputs=autoencoder.layers[0].output)\n",
    "encoded_features = np.array(encoder_layer.predict(X_train_scaled))\n",
    "print('Encoded feature covariance\\n', np.round(np.cov(encoded_features.T), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reconstrunction error\n",
      " 0.024166302454890175\n",
      "Test reconstrunction error\n",
      " 0.022729708452168602\n"
     ]
    }
   ],
   "source": [
    "train_predictions = autoencoder.predict(X_train_scaled)\n",
    "print('Train reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_train_scaled, train_predictions))\n",
    "test_predictions = autoencoder.predict(X_test_scaled)\n",
    "print('Test reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_test_scaled, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Constraint: Unit Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Unit Norm constraint on Encoding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 2)                 12        \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 5)                 15        \n",
      "=================================================================\n",
      "Total params: 27\n",
      "Trainable params: 27\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f24b8156dd0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Dense(encoding_dim, activation=\"linear\", input_shape=(input_dim,), use_bias = True, kernel_constraint=UnitNorm(axis=0))\n",
    "decoder = Dense(input_dim, activation=\"linear\", use_bias = True)\n",
    "\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "\n",
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='sgd')\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled,\n",
    "                epochs=nb_epoch,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder weights norm, \n",
      " [1.006 1.006]\n"
     ]
    }
   ],
   "source": [
    "w_encoder = np.round(autoencoder.layers[0].get_weights()[0], 2).T  # W in Figure 2.\n",
    "print('Encoder weights norm, \\n', np.round(np.sum(w_encoder ** 2, axis = 1),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reconstrunction error\n",
      " 0.02233539208002045\n",
      "Test reconstrunction error\n",
      " 0.02182826257448169\n"
     ]
    }
   ],
   "source": [
    "train_predictions = autoencoder.predict(X_train_scaled)\n",
    "print('Train reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_train_scaled, train_predictions))\n",
    "test_predictions = autoencoder.predict(X_test_scaled)\n",
    "print('Test reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_test_scaled, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Unit Norm constraint on both Encoding and Decoding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 2)                 12        \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 5)                 15        \n",
      "=================================================================\n",
      "Total params: 27\n",
      "Trainable params: 27\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f24b806a9d0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Dense(encoding_dim, activation=\"linear\", input_shape=(input_dim,), use_bias = True, kernel_constraint=UnitNorm(axis=0))\n",
    "decoder = Dense(input_dim, activation=\"linear\", use_bias = True, kernel_constraint=UnitNorm(axis=1))\n",
    "\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "\n",
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='sgd')\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled,\n",
    "                epochs=nb_epoch,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder weights norm, \n",
      " [0.999 1.004]\n",
      "Decoder weights norm, \n",
      " [0.996 0.993]\n"
     ]
    }
   ],
   "source": [
    "w_encoder = np.round(autoencoder.layers[0].get_weights()[0], 2).T  # W in Figure 2.\n",
    "w_decoder = np.round(autoencoder.layers[1].get_weights()[0], 2)  # W' in Figure 2.\n",
    "\n",
    "print('Encoder weights norm, \\n', np.round(np.sum(w_encoder ** 2, axis = 1),3))\n",
    "print('Decoder weights norm, \\n', np.round(np.sum(w_decoder ** 2, axis = 1),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reconstrunction error\n",
      " 0.015216757583960159\n",
      "Test reconstrunction error\n",
      " 0.014341794905161442\n"
     ]
    }
   ],
   "source": [
    "train_predictions = autoencoder.predict(X_train_scaled)\n",
    "print('Train reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_train_scaled, train_predictions))\n",
    "test_predictions = autoencoder.predict(X_test_scaled)\n",
    "print('Test reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_test_scaled, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraints put together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 2)                 12        \n",
      "_________________________________________________________________\n",
      "dense_tied_3 (DenseTied)     (None, 5)                 22        \n",
      "=================================================================\n",
      "Total params: 22\n",
      "Trainable params: 12\n",
      "Non-trainable params: 10\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f249051b1d0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Dense(encoding_dim, activation=\"linear\", input_shape=(input_dim,), use_bias = True, kernel_regularizer=WeightsOrthogonalityConstraint(encoding_dim, weightage=1., axis=0), kernel_constraint=UnitNorm(axis=0))\n",
    "decoder = DenseTied(input_dim, activation=\"linear\", tied_to=encoder, use_bias = False)\n",
    "\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "\n",
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='sgd')\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled,\n",
    "                epochs=nb_epoch,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reconstrunction error\n",
      " 0.00848629044050973\n",
      "Test reconstrunction error\n",
      " 0.008613352028043192\n"
     ]
    }
   ],
   "source": [
    "train_predictions = autoencoder.predict(X_train_scaled)\n",
    "print('Train reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_train_scaled, train_predictions))\n",
    "test_predictions = autoencoder.predict(X_test_scaled)\n",
    "print('Test reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_test_scaled, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:hydrogen",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
